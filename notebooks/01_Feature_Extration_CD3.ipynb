{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61df6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc7d6b",
   "metadata": {},
   "source": [
    "generate extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689033c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Path added: D:\\Minor Project Ayan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5702013 , 0.8906616 , 0.40881008, ..., 0.54290503, 0.3509516 ,\n",
       "        0.57402194],\n",
       "       [1.5678593 , 0.87527925, 0.42430186, ..., 0.36986795, 0.39370158,\n",
       "        0.33279702],\n",
       "       [1.2369174 , 0.6940583 , 0.46659434, ..., 0.30672607, 0.49007007,\n",
       "        0.34028953],\n",
       "       ...,\n",
       "       [1.6662837 , 0.84106517, 0.4418934 , ..., 0.5299991 , 0.3423527 ,\n",
       "        0.532659  ],\n",
       "       [1.4092584 , 0.89826334, 0.34626147, ..., 0.39969313, 0.44348124,\n",
       "        0.37054908],\n",
       "       [1.5877883 , 0.77838045, 0.4565112 , ..., 0.3411828 , 0.45100173,\n",
       "        0.2755862 ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path (adjust if your structure differs)\n",
    "project_root = r\"D:\\Minor Project Ayan\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Check it's added correctly\n",
    "print(\"✅ Path added:\", project_root)\n",
    "\n",
    "# Now import the module\n",
    "from utils.c3d_feature_extractor import extract_c3d_features\n",
    "\n",
    "\n",
    "video_path = r\"D:\\Minor Project Ayan\\data\\Abuse001_x264.mp4\"\n",
    "\n",
    "# Save as .npy\n",
    "extract_c3d_features(video_path,\n",
    "                    save_path=r\"D:\\Minor Project Ayan\\data\\C3D_Features\\Abuse001_features.npy\",\n",
    "                    use_cuda=True,\n",
    "                    save_format=\"npy\")\n",
    "\n",
    "# Save as .mat\n",
    "extract_c3d_features(video_path,\n",
    "                    save_path=r\"D:\\Minor Project Ayan\\data\\C3D_Features\\Abuse001_features.mat\",\n",
    "                    use_cuda=True,\n",
    "                    save_format=\"mat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b1050",
   "metadata": {},
   "source": [
    "feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86f1aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos from 'Stealing'...\n",
      "Processing video: Stealing002_x264.mp4\n",
      "Processing video: Stealing003_x264.mp4\n",
      "Processing video: Stealing004_x264.mp4\n",
      "Processing video: Stealing006_x264.mp4\n",
      "Processing video: Stealing007_x264.mp4\n",
      "Processing video: Stealing008_x264.mp4\n",
      "Processing video: Stealing009_x264.mp4\n",
      "Processing video: Stealing010_x264.mp4\n",
      "Processing video: Stealing011_x264.mp4\n",
      "Processing video: Stealing012_x264.mp4\n",
      "Processing video: Stealing013_x264.mp4\n",
      "Processing video: Stealing014_x264.mp4\n",
      "Processing video: Stealing015_x264.mp4\n",
      "Processing video: Stealing016_x264.mp4\n",
      "Processing video: Stealing017_x264.mp4\n",
      "Processing video: Stealing018_x264.mp4\n",
      "Processing video: Stealing019_x264.mp4\n",
      "Processing video: Stealing020_x264.mp4\n",
      "Processing video: Stealing021_x264.mp4\n",
      "Processing video: Stealing022_x264.mp4\n",
      "Processing video: Stealing023_x264.mp4\n",
      "Processing video: Stealing024_x264.mp4\n",
      "Processing video: Stealing025_x264.mp4\n",
      "Processing video: Stealing026_x264.mp4\n",
      "Processing video: Stealing027_x264.mp4\n",
      "Processing video: Stealing028_x264.mp4\n",
      "Processing video: Stealing029_x264.mp4\n",
      "Processing video: Stealing030_x264.mp4\n",
      "Processing video: Stealing031_x264.mp4\n",
      "Processing video: Stealing032_x264.mp4\n",
      "Processing video: Stealing035_x264.mp4\n",
      "Processing video: Stealing036_x264.mp4\n",
      "Processing video: Stealing037_x264.mp4\n",
      "Processing video: Stealing042_x264.mp4\n",
      "Processing video: Stealing043_x264.mp4\n",
      "Processing video: Stealing044_x264.mp4\n",
      "Processing video: Stealing045_x264.mp4\n",
      "Processing video: Stealing046_x264.mp4\n",
      "Processing video: Stealing047_x264.mp4\n",
      "Processing video: Stealing048_x264.mp4\n",
      "Processing videos from 'Vandalism'...\n",
      "Processing video: Vandalism001_x264.mp4\n",
      "Processing video: Vandalism002_x264.mp4\n",
      "Processing video: Vandalism003_x264.mp4\n",
      "Processing video: Vandalism004_x264.mp4\n",
      "Processing video: Vandalism005_x264.mp4\n",
      "Processing video: Vandalism006_x264.mp4\n",
      "Processing video: Vandalism007_x264.mp4\n",
      "Processing video: Vandalism008_x264.mp4\n",
      "Processing video: Vandalism009_x264.mp4\n",
      "Processing video: Vandalism010_x264.mp4\n",
      "Processing video: Vandalism011_x264.mp4\n",
      "Processing video: Vandalism012_x264.mp4\n",
      "Processing video: Vandalism013_x264.mp4\n",
      "Processing video: Vandalism014_x264.mp4\n",
      "Processing video: Vandalism015_x264.mp4\n",
      "Processing video: Vandalism016_x264.mp4\n",
      "Processing video: Vandalism017_x264.mp4\n",
      "Processing video: Vandalism018_x264.mp4\n",
      "Processing video: Vandalism019_x264.mp4\n",
      "Processing video: Vandalism020_x264.mp4\n",
      "Processing video: Vandalism021_x264.mp4\n",
      "Processing video: Vandalism022_x264.mp4\n",
      "Processing video: Vandalism023_x264.mp4\n",
      "Processing video: Vandalism024_x264.mp4\n",
      "Processing video: Vandalism025_x264.mp4\n",
      "Processing video: Vandalism026_x264.mp4\n",
      "Processing video: Vandalism027_x264.mp4\n",
      "Processing video: Vandalism028_x264.mp4\n",
      "Processing video: Vandalism029_x264.mp4\n",
      "Processing video: Vandalism030_x264.mp4\n",
      "Processing video: Vandalism031_x264.mp4\n",
      "Processing video: Vandalism032_x264.mp4\n",
      "Processing video: Vandalism033_x264.mp4\n",
      "Processing video: Vandalism034_x264.mp4\n",
      "Processing video: Vandalism035_x264.mp4\n",
      "Processing video: Vandalism036_x264.mp4\n",
      "Processing video: Vandalism037_x264.mp4\n",
      "Processing video: Vandalism038_x264.mp4\n",
      "Processing video: Vandalism039_x264.mp4\n",
      "Processing video: Vandalism040_x264.mp4\n",
      "Processing video: Vandalism041_x264.mp4\n",
      "Processing video: Vandalism042_x264.mp4\n",
      "Processing video: Vandalism043_x264.mp4\n",
      "Processing video: Vandalism044_x264.mp4\n",
      "Processing video: Vandalism045_x264.mp4\n",
      "Processing video: Vandalism046_x264.mp4\n",
      "Processing video: Vandalism047_x264.mp4\n",
      "Processing video: Vandalism048_x264.mp4\n",
      "Processing video: Vandalism049_x264.mp4\n",
      "Processing video: Vandalism050_x264.mp4\n",
      "Feature extraction completed for all videos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.c3d_feature_extractor import extract_c3d_features\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = r\"D:\\Minor Project Ayan\\dataset_Extras\"  # Update with your dataset path\n",
    "\n",
    "# Subfolder names for each class\n",
    "subfolders = [\"Stealing\",\"Vandalism\"]\n",
    "\n",
    "# Define the path where the extracted features will be saved\n",
    "output_path = r\"D:\\Minor Project Ayan\\data\\C3D_Features\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Process all videos in the dataset\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(dataset_path, subfolder)\n",
    "    print(f\"Processing videos from '{subfolder}'...\")\n",
    "\n",
    "    # Iterate over all videos in the subfolder\n",
    "    for video_name in os.listdir(subfolder_path):\n",
    "        video_path = os.path.join(subfolder_path, video_name)\n",
    "        torch.cuda.empty_cache()\n",
    "        # Ensure the file is a video (check file extension)\n",
    "        if video_path.endswith(('.mp4', '.avi', '.mov')):\n",
    "            print(f\"Processing video: {video_name}\")\n",
    "            \n",
    "            # Define the path to save extracted features\n",
    "            save_path = os.path.join(output_path, f\"{subfolder}_{video_name.split('.')[0]}_features.npy\")\n",
    "            \n",
    "            # Extract C3D features and save them\n",
    "            extract_c3d_features(video_path, save_path=save_path, use_cuda=True, save_format=\"npy\")\n",
    "\n",
    "print(\"Feature extraction completed for all videos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d79810",
   "metadata": {},
   "source": [
    "prepare train data {x_Train.npy and y_Train_npy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87044c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping unknown class in file: Abuse001_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting001_x264_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting004_x264_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting005_x264_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting006_x264_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting007_x264_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting009_x264_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting010_x264_features.npy\n",
      "[WARNING] Skipping unknown class in file: Shoplifting_Shoplifting013_x264_features.npy\n",
      "[INFO] Saved 597 feature arrays and labels.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory where extracted features are saved\n",
    "features_dir = r\"D:\\Minor Project Ayan\\data\\C3D_Features\"\n",
    "\n",
    "# Output files\n",
    "features_output = r\"D:\\Minor Project Ayan\\data\\X_train.npy\"\n",
    "labels_output = r\"D:\\Minor Project Ayan\\data\\y_train.npy\"\n",
    "\n",
    "class_mapping = {\n",
    "    \"Abuse\": 0,\n",
    "    \"Arrest\": 1,\n",
    "    \"Arson\": 2,\n",
    "    \"Assault\": 3,\n",
    "    \"Burglary\": 4,\n",
    "    \"Explosion\": 5,\n",
    "    \"Fighting\": 6,\n",
    "    \"Normal\": 7,\n",
    "    \"RoadAccidents\": 8,\n",
    "    \"Robbery\": 9,\n",
    "    \"Shooting\": 10,\n",
    "    \"Stealing\": 11,\n",
    "    \"Vandalism\": 12\n",
    "}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Loop over all .npy feature files\n",
    "for file_name in os.listdir(features_dir):\n",
    "    if not file_name.endswith(\"_features.npy\"):\n",
    "        continue\n",
    "\n",
    "    # Example: 'Abuse_Abuse001_x264_features.npy' → 'Abuse'\n",
    "    label_name = file_name.split(\"_\")[0]\n",
    "    label = class_mapping.get(label_name)\n",
    "\n",
    "    if label is None:\n",
    "        print(f\"[WARNING] Skipping unknown class in file: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    feature_path = os.path.join(features_dir, file_name)\n",
    "    features = np.load(feature_path)\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train, dtype=object)  # Keep object dtype for variable-length segments\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Save them\n",
    "np.save(features_output, X_train)\n",
    "np.save(labels_output, y_train)\n",
    "\n",
    "print(f\"[INFO] Saved {len(X_train)} feature arrays and labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69681b95",
   "metadata": {},
   "source": [
    "training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb01359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dtype: float32, shape: (597, 32, 512)\n",
      "y_train dtype: int64, shape: (597,)\n",
      "First element in X_train: [[1.5702013  0.8906616  0.40881008 ... 0.54290503 0.3509516  0.57402194]\n",
      " [1.5678593  0.87527925 0.42430186 ... 0.36986795 0.39370158 0.33279702]\n",
      " [1.2369174  0.6940583  0.46659434 ... 0.30672607 0.49007007 0.34028953]\n",
      " ...\n",
      " [1.6662837  0.84106517 0.4418934  ... 0.5299991  0.3423527  0.532659  ]\n",
      " [1.4092584  0.89826334 0.34626147 ... 0.39969313 0.44348124 0.37054908]\n",
      " [1.5877883  0.77838045 0.4565112  ... 0.3411828  0.45100173 0.2755862 ]]\n",
      "First element in y_train: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">328,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m328,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">381,825</span> (1.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m381,825\u001b[0m (1.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">381,825</span> (1.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m381,825\u001b[0m (1.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0894 - loss: -29.1110\n",
      "Epoch 1: val_loss improved from inf to -330.33655, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.0893 - loss: -30.2137 - val_accuracy: 0.0000e+00 - val_loss: -330.3365\n",
      "Epoch 2/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0855 - loss: -186.6964\n",
      "Epoch 2: val_loss improved from -330.33655 to -847.19043, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.0855 - loss: -187.5100 - val_accuracy: 0.0000e+00 - val_loss: -847.1904\n",
      "Epoch 3/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0688 - loss: -445.6006\n",
      "Epoch 3: val_loss improved from -847.19043 to -1662.23267, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0693 - loss: -447.7639 - val_accuracy: 0.0000e+00 - val_loss: -1662.2327\n",
      "Epoch 4/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0822 - loss: -813.5466\n",
      "Epoch 4: val_loss improved from -1662.23267 to -2794.18750, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.0822 - loss: -816.7283 - val_accuracy: 0.0000e+00 - val_loss: -2794.1875\n",
      "Epoch 5/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0784 - loss: -1334.0920\n",
      "Epoch 5: val_loss improved from -2794.18750 to -4252.33496, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.0787 - loss: -1342.1340 - val_accuracy: 0.0000e+00 - val_loss: -4252.3350\n",
      "Epoch 6/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0979 - loss: -1877.4785\n",
      "Epoch 6: val_loss improved from -4252.33496 to -6030.89746, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0970 - loss: -1894.4327 - val_accuracy: 0.0000e+00 - val_loss: -6030.8975\n",
      "Epoch 7/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0777 - loss: -2837.4714\n",
      "Epoch 7: val_loss improved from -6030.89746 to -8152.36084, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0778 - loss: -2839.5269 - val_accuracy: 0.0000e+00 - val_loss: -8152.3608\n",
      "Epoch 8/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0780 - loss: -3682.8616\n",
      "Epoch 8: val_loss improved from -8152.36084 to -10610.38867, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0783 - loss: -3698.7324 - val_accuracy: 0.0000e+00 - val_loss: -10610.3887\n",
      "Epoch 9/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0693 - loss: -4932.1357\n",
      "Epoch 9: val_loss improved from -10610.38867 to -13370.12500, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0698 - loss: -4935.0151 - val_accuracy: 0.0000e+00 - val_loss: -13370.1250\n",
      "Epoch 10/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0891 - loss: -5704.4507\n",
      "Epoch 10: val_loss improved from -13370.12500 to -16429.86719, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.0887 - loss: -5738.1753 - val_accuracy: 0.0000e+00 - val_loss: -16429.8672\n",
      "Epoch 11/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0880 - loss: -7215.8403\n",
      "Epoch 11: val_loss improved from -16429.86719 to -19830.76367, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0879 - loss: -7221.3911 - val_accuracy: 0.0000e+00 - val_loss: -19830.7637\n",
      "Epoch 12/60\n",
      "\u001b[1m56/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0632 - loss: -8457.4639\n",
      "Epoch 12: val_loss improved from -19830.76367 to -23493.15430, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0650 - loss: -8505.3262 - val_accuracy: 0.0000e+00 - val_loss: -23493.1543\n",
      "Epoch 13/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0939 - loss: -9943.0312\n",
      "Epoch 13: val_loss improved from -23493.15430 to -27469.30078, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0936 - loss: -9965.3193 - val_accuracy: 0.0000e+00 - val_loss: -27469.3008\n",
      "Epoch 14/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0935 - loss: -12020.0020\n",
      "Epoch 14: val_loss improved from -27469.30078 to -31708.32617, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0930 - loss: -12040.9160 - val_accuracy: 0.0000e+00 - val_loss: -31708.3262\n",
      "Epoch 15/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1067 - loss: -13477.6953\n",
      "Epoch 15: val_loss improved from -31708.32617 to -36184.03906, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1053 - loss: -13514.7891 - val_accuracy: 0.0000e+00 - val_loss: -36184.0391\n",
      "Epoch 16/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1037 - loss: -15484.0752\n",
      "Epoch 16: val_loss improved from -36184.03906 to -40975.41797, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1030 - loss: -15504.6846 - val_accuracy: 0.0000e+00 - val_loss: -40975.4180\n",
      "Epoch 17/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0906 - loss: -17116.5469\n",
      "Epoch 17: val_loss improved from -40975.41797 to -45994.53906, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0905 - loss: -17134.0117 - val_accuracy: 0.0000e+00 - val_loss: -45994.5391\n",
      "Epoch 18/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0790 - loss: -19814.7793\n",
      "Epoch 18: val_loss improved from -45994.53906 to -51316.57812, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0793 - loss: -19839.0312 - val_accuracy: 0.0000e+00 - val_loss: -51316.5781\n",
      "Epoch 19/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0878 - loss: -20911.2148\n",
      "Epoch 19: val_loss improved from -51316.57812 to -56822.31641, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0875 - loss: -21019.9121 - val_accuracy: 0.0000e+00 - val_loss: -56822.3164\n",
      "Epoch 20/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0859 - loss: -24900.4473\n",
      "Epoch 20: val_loss improved from -56822.31641 to -62651.05078, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0858 - loss: -24898.2793 - val_accuracy: 0.0000e+00 - val_loss: -62651.0508\n",
      "Epoch 21/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1078 - loss: -25482.0898\n",
      "Epoch 21: val_loss improved from -62651.05078 to -68622.78125, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1070 - loss: -25552.5625 - val_accuracy: 0.0000e+00 - val_loss: -68622.7812\n",
      "Epoch 22/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0755 - loss: -28276.2422\n",
      "Epoch 22: val_loss improved from -68622.78125 to -74909.63281, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0759 - loss: -28371.1172 - val_accuracy: 0.0000e+00 - val_loss: -74909.6328\n",
      "Epoch 23/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0843 - loss: -30401.6309\n",
      "Epoch 23: val_loss improved from -74909.63281 to -81427.79688, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0843 - loss: -30437.1797 - val_accuracy: 0.0000e+00 - val_loss: -81427.7969\n",
      "Epoch 24/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0951 - loss: -36076.9023\n",
      "Epoch 24: val_loss improved from -81427.79688 to -88291.19531, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0944 - loss: -36043.4414 - val_accuracy: 0.0000e+00 - val_loss: -88291.1953\n",
      "Epoch 25/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0881 - loss: -38227.4375\n",
      "Epoch 25: val_loss improved from -88291.19531 to -95235.34375, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0879 - loss: -38230.8047 - val_accuracy: 0.0000e+00 - val_loss: -95235.3438\n",
      "Epoch 26/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0934 - loss: -38675.0898\n",
      "Epoch 26: val_loss improved from -95235.34375 to -102348.67969, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0930 - loss: -38756.3867 - val_accuracy: 0.0000e+00 - val_loss: -102348.6797\n",
      "Epoch 27/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1012 - loss: -40804.2031\n",
      "Epoch 27: val_loss improved from -102348.67969 to -109690.38281, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.1009 - loss: -40862.1875 - val_accuracy: 0.0000e+00 - val_loss: -109690.3828\n",
      "Epoch 28/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0902 - loss: -45312.1289\n",
      "Epoch 28: val_loss improved from -109690.38281 to -117448.82031, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0897 - loss: -45479.3047 - val_accuracy: 0.0000e+00 - val_loss: -117448.8203\n",
      "Epoch 29/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0778 - loss: -53337.3594\n",
      "Epoch 29: val_loss improved from -117448.82031 to -125286.61719, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0779 - loss: -53295.5898 - val_accuracy: 0.0000e+00 - val_loss: -125286.6172\n",
      "Epoch 30/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0789 - loss: -54881.6250\n",
      "Epoch 30: val_loss improved from -125286.61719 to -133363.20312, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0792 - loss: -54837.6914 - val_accuracy: 0.0000e+00 - val_loss: -133363.2031\n",
      "Epoch 31/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0851 - loss: -56037.3047\n",
      "Epoch 31: val_loss improved from -133363.20312 to -141505.79688, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0850 - loss: -56132.4375 - val_accuracy: 0.0000e+00 - val_loss: -141505.7969\n",
      "Epoch 32/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0664 - loss: -61968.5977\n",
      "Epoch 32: val_loss improved from -141505.79688 to -150002.70312, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0675 - loss: -61914.4922 - val_accuracy: 0.0000e+00 - val_loss: -150002.7031\n",
      "Epoch 33/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0681 - loss: -65810.7812\n",
      "Epoch 33: val_loss improved from -150002.70312 to -158699.26562, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0684 - loss: -65783.1172 - val_accuracy: 0.0000e+00 - val_loss: -158699.2656\n",
      "Epoch 34/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0981 - loss: -67012.5469\n",
      "Epoch 34: val_loss improved from -158699.26562 to -167493.48438, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0978 - loss: -67028.1016 - val_accuracy: 0.0000e+00 - val_loss: -167493.4844\n",
      "Epoch 35/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0706 - loss: -67517.4766\n",
      "Epoch 35: val_loss improved from -167493.48438 to -176602.82812, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0713 - loss: -67715.0859 - val_accuracy: 0.0000e+00 - val_loss: -176602.8281\n",
      "Epoch 36/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0944 - loss: -73298.5000\n",
      "Epoch 36: val_loss improved from -176602.82812 to -186286.92188, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0938 - loss: -73447.0859 - val_accuracy: 0.0000e+00 - val_loss: -186286.9219\n",
      "Epoch 37/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0773 - loss: -82707.2734\n",
      "Epoch 37: val_loss improved from -186286.92188 to -195948.59375, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0774 - loss: -82657.0156 - val_accuracy: 0.0000e+00 - val_loss: -195948.5938\n",
      "Epoch 38/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0688 - loss: -83215.5938\n",
      "Epoch 38: val_loss improved from -195948.59375 to -205840.07812, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0690 - loss: -83234.5859 - val_accuracy: 0.0000e+00 - val_loss: -205840.0781\n",
      "Epoch 39/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1015 - loss: -83987.3594\n",
      "Epoch 39: val_loss improved from -205840.07812 to -215863.31250, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1004 - loss: -84271.0469 - val_accuracy: 0.0000e+00 - val_loss: -215863.3125\n",
      "Epoch 40/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1016 - loss: -84562.1406\n",
      "Epoch 40: val_loss improved from -215863.31250 to -226087.32812, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1005 - loss: -85089.3047 - val_accuracy: 0.0000e+00 - val_loss: -226087.3281\n",
      "Epoch 41/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0735 - loss: -96827.1875\n",
      "Epoch 41: val_loss improved from -226087.32812 to -236623.84375, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0738 - loss: -96843.3281 - val_accuracy: 0.0000e+00 - val_loss: -236623.8438\n",
      "Epoch 42/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0905 - loss: -96618.5156\n",
      "Epoch 42: val_loss improved from -236623.84375 to -247275.70312, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0900 - loss: -96931.0000 - val_accuracy: 0.0000e+00 - val_loss: -247275.7031\n",
      "Epoch 43/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0715 - loss: -110318.7578\n",
      "Epoch 43: val_loss improved from -247275.70312 to -258264.09375, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0723 - loss: -110041.6172 - val_accuracy: 0.0000e+00 - val_loss: -258264.0938\n",
      "Epoch 44/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0652 - loss: -110766.1172\n",
      "Epoch 44: val_loss improved from -258264.09375 to -269250.09375, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0662 - loss: -110780.4375 - val_accuracy: 0.0000e+00 - val_loss: -269250.0938\n",
      "Epoch 45/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0950 - loss: -114751.6406\n",
      "Epoch 45: val_loss improved from -269250.09375 to -280256.15625, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0946 - loss: -114754.9141 - val_accuracy: 0.0000e+00 - val_loss: -280256.1562\n",
      "Epoch 46/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0957 - loss: -118982.6875\n",
      "Epoch 46: val_loss improved from -280256.15625 to -291889.56250, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0955 - loss: -118999.3359 - val_accuracy: 0.0000e+00 - val_loss: -291889.5625\n",
      "Epoch 47/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0934 - loss: -123722.3516\n",
      "Epoch 47: val_loss improved from -291889.56250 to -303526.46875, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0931 - loss: -123759.6953 - val_accuracy: 0.0000e+00 - val_loss: -303526.4688\n",
      "Epoch 48/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0641 - loss: -130547.9531\n",
      "Epoch 48: val_loss improved from -303526.46875 to -315164.18750, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0654 - loss: -130480.0391 - val_accuracy: 0.0000e+00 - val_loss: -315164.1875\n",
      "Epoch 49/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0639 - loss: -138878.3438\n",
      "Epoch 49: val_loss improved from -315164.18750 to -327050.50000, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0642 - loss: -138797.3750 - val_accuracy: 0.0000e+00 - val_loss: -327050.5000\n",
      "Epoch 50/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0979 - loss: -136827.6562\n",
      "Epoch 50: val_loss improved from -327050.50000 to -338954.53125, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0977 - loss: -136872.1406 - val_accuracy: 0.0000e+00 - val_loss: -338954.5312\n",
      "Epoch 51/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0654 - loss: -144991.9219\n",
      "Epoch 51: val_loss improved from -338954.53125 to -351158.46875, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0666 - loss: -144934.7500 - val_accuracy: 0.0000e+00 - val_loss: -351158.4688\n",
      "Epoch 52/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0894 - loss: -146808.1406\n",
      "Epoch 52: val_loss improved from -351158.46875 to -363522.31250, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0890 - loss: -146987.2812 - val_accuracy: 0.0000e+00 - val_loss: -363522.3125\n",
      "Epoch 53/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0785 - loss: -156873.4062\n",
      "Epoch 53: val_loss improved from -363522.31250 to -376287.65625, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0786 - loss: -156846.9531 - val_accuracy: 0.0000e+00 - val_loss: -376287.6562\n",
      "Epoch 54/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0883 - loss: -151055.5312\n",
      "Epoch 54: val_loss improved from -376287.65625 to -388910.59375, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0882 - loss: -151195.7656 - val_accuracy: 0.0000e+00 - val_loss: -388910.5938\n",
      "Epoch 55/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0744 - loss: -161477.8281\n",
      "Epoch 55: val_loss improved from -388910.59375 to -401906.37500, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0748 - loss: -161736.3594 - val_accuracy: 0.0000e+00 - val_loss: -401906.3750\n",
      "Epoch 56/60\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1062 - loss: -169117.6250\n",
      "Epoch 56: val_loss improved from -401906.37500 to -415103.09375, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1055 - loss: -169178.1406 - val_accuracy: 0.0000e+00 - val_loss: -415103.0938\n",
      "Epoch 57/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0812 - loss: -174859.7188\n",
      "Epoch 57: val_loss improved from -415103.09375 to -428452.37500, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0814 - loss: -175038.4375 - val_accuracy: 0.0000e+00 - val_loss: -428452.3750\n",
      "Epoch 58/60\n",
      "\u001b[1m58/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0540 - loss: -180964.5625\n",
      "Epoch 58: val_loss improved from -428452.37500 to -441875.62500, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0554 - loss: -181003.6250 - val_accuracy: 0.0000e+00 - val_loss: -441875.6250\n",
      "Epoch 59/60\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0762 - loss: -187090.0781\n",
      "Epoch 59: val_loss improved from -441875.62500 to -455343.65625, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0764 - loss: -187120.0938 - val_accuracy: 0.0000e+00 - val_loss: -455343.6562\n",
      "Epoch 60/60\n",
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0783 - loss: -201982.1250\n",
      "Epoch 60: val_loss improved from -455343.65625 to -469111.12500, saving model to D:/Minor Project Ayan/models/best_model.keras\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0787 - loss: -201404.5000 - val_accuracy: 0.0000e+00 - val_loss: -469111.1250\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "# Load the data\n",
    "X_train = np.load('D:/Minor Project Ayan/data/X_train.npy', allow_pickle=True)\n",
    "y_train = np.load('D:/Minor Project Ayan/data/y_train.npy', allow_pickle=True)\n",
    "\n",
    "# Convert to proper dtype\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.int64)\n",
    "\n",
    "print(f\"X_train dtype: {X_train.dtype}, shape: {X_train.shape}\")\n",
    "print(f\"y_train dtype: {y_train.dtype}, shape: {y_train.shape}\")\n",
    "print(f\"First element in X_train: {X_train[0]}\")\n",
    "print(f\"First element in y_train: {y_train[0]}\")\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(32, 512), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_dir = \"D:/Minor Project Ayan/models/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model.keras'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=os.path.join(checkpoint_dir, 'logs'),\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=60,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(os.path.join(checkpoint_dir, 'final_model.keras'))\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35843865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Model Training Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Load C3D model\n",
    "def load_c3d_model():\n",
    "    model_json_path = \"D:\\Minor Project Ayan\\models\\model.json\"\n",
    "    model_weights_path = \"D:\\Minor Project Ayan\\models\\weights_L1L2.mat\"\n",
    "\n",
    "    with open(model_json_path, 'r') as json_file:\n",
    "        c3d_model = model_from_json(json_file.read())\n",
    "\n",
    "    import h5py\n",
    "    import scipy.io\n",
    "    mat = scipy.io.loadmat(model_weights_path)\n",
    "    weights = mat['weights'][0]\n",
    "\n",
    "    for layer, w in zip(c3d_model.layers, weights):\n",
    "        if hasattr(layer, 'set_weights'):\n",
    "            layer.set_weights([w[0], w[1]])\n",
    "\n",
    "    return c3d_model\n",
    "\n",
    "# Resize + preprocess frame\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.resize(frame, (112, 112))\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame -= 128\n",
    "    return frame\n",
    "\n",
    "# Extract 32 segments from the video and feed to C3D\n",
    "def extract_c3d_features(video_path, c3d_model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = preprocess_frame(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    total_frames = len(frames)\n",
    "    if total_frames < 16:\n",
    "        raise ValueError(\"Video too short to extract C3D features.\")\n",
    "\n",
    "    # Split video into 32 equal segments (each of 16 frames)\n",
    "    segment_size = total_frames // 32\n",
    "    features = []\n",
    "\n",
    "    for i in range(32):\n",
    "        start = i * segment_size\n",
    "        end = start + 16\n",
    "        if end > total_frames:\n",
    "            segment = frames[-16:]\n",
    "        else:\n",
    "            segment = frames[start:end]\n",
    "\n",
    "        segment = np.array(segment)\n",
    "        segment = np.expand_dims(segment, axis=0)  # shape: (1, 16, 112, 112, 3)\n",
    "        feature = c3d_model.predict(segment)[0]\n",
    "        features.append(feature)\n",
    "\n",
    "    return np.array(features)  # shape: (32, 512)\n",
    "\n",
    "# Inference using trained LSTM model\n",
    "def predict_anomaly(features):\n",
    "    lstm_model_path = \"D:/Minor Project Ayan/models/best_model.keras\"\n",
    "    model = load_model(lstm_model_path)\n",
    "\n",
    "    features = np.expand_dims(features, axis=0)  # shape: (1, 32, 512)\n",
    "    prediction = model.predict(features)[0][0]\n",
    "\n",
    "    label = \"Anomalous\" if prediction >= 0.5 else \"Normal\"\n",
    "    return prediction, label\n",
    "\n",
    "# Main function\n",
    "def run_inference(video_path):\n",
    "    print(f\"Loading video: {video_path}\")\n",
    "    c3d_model = load_c3d_model()\n",
    "    features = extract_c3d_features(video_path, c3d_model)\n",
    "    score, label = predict_anomaly(features)\n",
    "    print(f\"Prediction Score: {score:.4f} → {label}\")\n",
    "\n",
    "# Run this to test\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"D:/Minor Project Ayan/data/Abuse001_x264.mp4\"  # change as needed\n",
    "    run_inference(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8dc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
